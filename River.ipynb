{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rikyr\\AppData\\Local\\Temp\\ipykernel_21872\\2973528257.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['ClientGender'].fillna('Unknown', inplace=True)\n",
      "C:\\Users\\rikyr\\AppData\\Local\\Temp\\ipykernel_21872\\2973528257.py:29: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['DaysSinceLastTransaction'].replace(0, 900, inplace=True)\n",
      "C:\\Users\\rikyr\\AppData\\Local\\Temp\\ipykernel_21872\\2973528257.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['AverageFrequencySoFar'].replace(900, 0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing top 1000 products by total quantity sold...\n",
      "After top-1000 undersampling:\n",
      "  - data has 484625 rows\n",
      "  - stocks_df has 3667 rows\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'learn_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 120\u001b[0m\n\u001b[0;32m    117\u001b[0m warmup_full \u001b[38;5;241m=\u001b[39m warmup_full\u001b[38;5;241m.\u001b[39msample(frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# Initialize the River model using BiasedMF (matrix factorization with biases)\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mreco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBiasedMF\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_factors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearn_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    124\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# Warm-up training\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m tqdm(warmup_full\u001b[38;5;241m.\u001b[39miterrows(), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(warmup_full), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarmup Training\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'learn_rate'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "from river import reco, metrics\n",
    "import multiprocessing\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load Data\n",
    "# -----------------------------\n",
    "data = pd.read_parquet(\"final_df.parquet\")\n",
    "clients_df = pd.read_csv(\"clients_dataset.csv\")\n",
    "stocks_df = pd.read_csv(\"stocks_dataset.csv\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Preprocess Data (Time-based)\n",
    "# -----------------------------\n",
    "data = data.sort_values(\"TransactionDate\").reset_index(drop=True)\n",
    "\n",
    "for col in ['Category', 'FamilyLevel1', 'FamilyLevel2', 'Brand', 'StoreCountry']:\n",
    "    if col not in stocks_df.columns:\n",
    "        stocks_df[col] = \"Unknown\"\n",
    "if 'StoreID' not in stocks_df.columns:\n",
    "    stocks_df['StoreID'] = \"0\"\n",
    "\n",
    "data['ClientGender'].fillna('Unknown', inplace=True)\n",
    "data['DaysSinceLastTransaction'].replace(0, 900, inplace=True)\n",
    "data['AverageFrequencySoFar'].replace(900, 0, inplace=True)\n",
    "data['Quarter'] = data['TransactionDate'].dt.quarter\n",
    "data = data.loc[:, ~data.columns.str.contains('Rolling90Pct')]\n",
    "\n",
    "# -----------------------------\n",
    "# 2a. Undersampling top 1000 sold products\n",
    "# -----------------------------\n",
    "print(\"Computing top 1000 products by total quantity sold...\")\n",
    "top_1000_products = (\n",
    "    data.groupby('ProductID')['Quantity_sold']\n",
    "    .sum()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(1000)\n",
    "    .index\n",
    ")\n",
    "top_1000_products = set(top_1000_products)\n",
    "\n",
    "# Filter both data and stocks_df to these top 1000 products\n",
    "data = data[data['ProductID'].isin(top_1000_products)].copy()\n",
    "stocks_df = stocks_df[stocks_df['ProductID'].isin(top_1000_products)].copy()\n",
    "\n",
    "print(f\"After top-1000 undersampling:\\n  - data has {len(data)} rows\\n  - stocks_df has {len(stocks_df)} rows\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Create a ProductID->Universe map\n",
    "# -----------------------------\n",
    "product_universe_map = data.groupby(\"ProductID\")[\"Universe\"].last().to_dict()\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Utility Functions\n",
    "# -----------------------------\n",
    "def generate_negative_samples(df, n_neg=10):\n",
    "    \"\"\"\n",
    "    Generate negative samples for implicit feedback data.\n",
    "    Sceglie fino a n_neg prodotti, tra quelli apparsi in df, che l'utente non ha acquistato.\n",
    "    \"\"\"\n",
    "    all_products = np.array(df['ProductID'].unique())\n",
    "    neg_samples = []\n",
    "    grouped = df.groupby(\"ClientID\")[\"ProductID\"].apply(set).to_dict()\n",
    "    for client_id, pos_products in grouped.items():\n",
    "        available_neg = np.setdiff1d(all_products, list(pos_products))\n",
    "        if len(available_neg) < n_neg:\n",
    "            sampled_neg = np.random.choice(available_neg, n_neg, replace=True)\n",
    "        else:\n",
    "            sampled_neg = np.random.choice(available_neg, n_neg, replace=False)\n",
    "        base_info = df[df[\"ClientID\"] == client_id].iloc[-1].to_dict()\n",
    "        for neg_product in sampled_neg:\n",
    "            row_dict = {**base_info, \"ProductID\": neg_product, \"Label\": 0}\n",
    "            neg_samples.append(row_dict)\n",
    "    return pd.DataFrame(neg_samples)\n",
    "\n",
    "def save_model_river(river_model, filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(river_model, f)\n",
    "\n",
    "def load_model_river(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# For evaluation: generate top-K recommendations for a given client\n",
    "def generate_top_k_for_client(river_model, client_id, top_k=5):\n",
    "    scores = []\n",
    "    for pid in stocks_df[\"ProductID\"]:\n",
    "        sc = river_model.predict_one((client_id, pid))\n",
    "        scores.append((pid, sc))\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [pid for pid, sc in scores[:top_k]]\n",
    "\n",
    "def generate_recommendations_day(client_id, day, n_recommendations, river_model):\n",
    "    # In this implementation, we ignore \"day\" for scoring and simply use the model to score (client, item) pairs.\n",
    "    return generate_top_k_for_client(river_model, client_id, top_k=n_recommendations)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Training & Evaluation using River\n",
    "# -----------------------------\n",
    "start_day = data['TransactionDate'].min().normalize()\n",
    "warmup_end = start_day + pd.Timedelta(days=30)\n",
    "\n",
    "warmup_data = data[data['TransactionDate'] < warmup_end]\n",
    "if warmup_data.empty:\n",
    "    raise ValueError(\"No data available for warm-up.\")\n",
    "\n",
    "# Prepare warm-up positives and negatives\n",
    "warmup_pos = warmup_data.copy()\n",
    "warmup_pos[\"Label\"] = 1\n",
    "warmup_neg = generate_negative_samples(warmup_data, n_neg=10)\n",
    "warmup_full = pd.concat([warmup_pos, warmup_neg], ignore_index=True)\n",
    "warmup_full = warmup_full.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Initialize the River model using BiasedMF (matrix factorization with biases)\n",
    "model = reco.BiasedMF(\n",
    "    n_factors=16,\n",
    "    learn_rate=0.01,\n",
    "    use_bias=True\n",
    ")\n",
    "\n",
    "# Warm-up training\n",
    "for _, row in tqdm(warmup_full.iterrows(), total=len(warmup_full), desc=\"Warmup Training\"):\n",
    "    user_id = row[\"ClientID\"]\n",
    "    item_id = row[\"ProductID\"]\n",
    "    rating = row[\"Label\"]\n",
    "    model = model.learn_one((user_id, item_id), rating)\n",
    "\n",
    "save_model_river(model, \"model_initial.pkl\")\n",
    "current_model_file = \"model_initial.pkl\"\n",
    "print(\"Initial model trained (warm-up).\")\n",
    "\n",
    "evaluation_results = []\n",
    "all_days = pd.date_range(\n",
    "    start=warmup_end.normalize(),\n",
    "    end=data['TransactionDate'].max().normalize(),\n",
    "    freq='W'\n",
    ")\n",
    "\n",
    "for day in all_days:\n",
    "    window_start = day - pd.Timedelta(days=89)\n",
    "    if window_start < warmup_end:\n",
    "        window_start = warmup_end\n",
    "    \n",
    "    train_subset = data[\n",
    "        (data['TransactionDate'] >= window_start) &\n",
    "        (data['TransactionDate'] <= day)\n",
    "    ]\n",
    "    if train_subset.empty:\n",
    "        continue\n",
    "    \n",
    "    # Load current model\n",
    "    model = load_model_river(current_model_file)\n",
    "    \n",
    "    day_pos = train_subset.copy()\n",
    "    day_pos[\"Label\"] = 1\n",
    "    day_neg = generate_negative_samples(train_subset, n_neg=10)\n",
    "    day_full = pd.concat([day_pos, day_neg], ignore_index=True)\n",
    "    day_full = day_full.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Update model on this window\n",
    "    for _, row in tqdm(day_full.iterrows(), total=len(day_full), desc=f\"Training until {day.date()}\"):\n",
    "        user_id = row[\"ClientID\"]\n",
    "        item_id = row[\"ProductID\"]\n",
    "        rating = row[\"Label\"]\n",
    "        model = model.learn_one((user_id, item_id), rating)\n",
    "    \n",
    "    updated_model_file = f\"model_{day.strftime('%Y%m%d')}.pkl\"\n",
    "    save_model_river(model, updated_model_file)\n",
    "    print(f\"Model updated (window up to {day.date()}) and saved as {updated_model_file}\")\n",
    "    current_model_file = updated_model_file\n",
    "    \n",
    "    # Evaluate on today's transactions\n",
    "    day_rows = data[data['TransactionDate'].dt.normalize() == day]\n",
    "    if day_rows.empty:\n",
    "        continue\n",
    "\n",
    "    client_actual = day_rows.groupby(\"ClientID\")[\"ProductID\"].apply(set).to_dict()\n",
    "    total_clients = len(client_actual)\n",
    "    \n",
    "    def evaluate_client(client_id):\n",
    "        recommended = generate_recommendations_day(client_id, day, n_recommendations=5, river_model=model)\n",
    "        return 1 if set(recommended).intersection(client_actual[client_id]) else 0\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=multiprocessing.cpu_count()) as executor:\n",
    "        results = list(executor.map(evaluate_client, client_actual.keys()))\n",
    "    correct_count = sum(results)\n",
    "    \n",
    "    day_accuracy = correct_count / total_clients if total_clients else np.nan\n",
    "    evaluation_results.append({\n",
    "        \"day\": day,\n",
    "        \"total_clients\": total_clients,\n",
    "        \"correct\": correct_count,\n",
    "        \"accuracy\": day_accuracy\n",
    "    })\n",
    "    print(f\"Day {day.date()} -> Accuracy: {day_accuracy:.4f}\")\n",
    "\n",
    "eval_df = pd.DataFrame(evaluation_results)\n",
    "print(\"\\nDaily evaluation results:\")\n",
    "print(eval_df)\n",
    "overall_acc = eval_df[\"correct\"].sum() / eval_df[\"total_clients\"].sum()\n",
    "print(\"Overall accuracy:\", overall_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
