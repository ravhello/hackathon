{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "XLearnLibraryNotFound",
     "evalue": "Cannot find xlearn Library in the candidate path",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXLearnLibraryNotFound\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxlearn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxl\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnotebook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\xlearn\\__init__.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m absolute_import\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxlearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     20\u001b[0m VERSION_FILE \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18m__file__\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVERSION\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(VERSION_FILE) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\xlearn\\xlearn.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mctypes\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _LIB, XLearnHandle\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _check_call, c_str\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mXLearn\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\xlearn\\base.py:34\u001b[0m\n\u001b[0;32m     31\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m lib\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# load the xlearn library globally\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m _LIB \u001b[38;5;241m=\u001b[39m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_call\u001b[39m(ret):\n\u001b[0;32m     37\u001b[0m \u001b[38;5;250m\t\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    This function will raise exception when error occurs.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03m        return value from API calls\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m\t\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\xlearn\\base.py:27\u001b[0m, in \u001b[0;36m_load_lib\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_lib\u001b[39m():\n\u001b[0;32m     26\u001b[0m \u001b[38;5;250m\t\u001b[39m\u001b[38;5;124;03m\"\"\"Load xlearn shared library\"\"\"\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \tlib_path \u001b[38;5;241m=\u001b[39m \u001b[43mfind_lib_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lib_path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     29\u001b[0m \t\t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\xlearn\\libpath.py:58\u001b[0m, in \u001b[0;36mfind_lib_path\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# From github issues, most of installation errors come from machines w/o compilers\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib_path:\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m XLearnLibraryNotFound(\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot find xlearn Library in the candidate path\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     60\u001b[0m         )\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lib_path\n",
      "\u001b[1;31mXLearnLibraryNotFound\u001b[0m: Cannot find xlearn Library in the candidate path"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xlearn as xl\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your parquet and CSV files\n",
    "data = pd.read_parquet(\"final_df.parquet\")\n",
    "clients_df = pd.read_csv(\"clients_dataset.csv\")\n",
    "stocks_df = pd.read_csv(\"stocks_dataset.csv\")\n",
    "\n",
    "# Sort by date\n",
    "data = data.sort_values(\"TransactionDate\").reset_index(drop=True)\n",
    "\n",
    "# Basic fill/clean\n",
    "data['ClientGender'].fillna('Unknown', inplace=True)\n",
    "data['DaysSinceLastTransaction'].replace(0, 900, inplace=True)\n",
    "data['AverageFrequencySoFar'].replace(900, 0, inplace=True)\n",
    "data['Quarter'] = data['TransactionDate'].dt.quarter\n",
    "\n",
    "# Example: restricting to top 1000 products\n",
    "top_1000_products = (\n",
    "    data.groupby('ProductID')['Quantity_sold']\n",
    "    .sum()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(1000)\n",
    "    .index\n",
    ")\n",
    "data = data[data['ProductID'].isin(top_1000_products)].copy()\n",
    "stocks_df = stocks_df[stocks_df['ProductID'].isin(top_1000_products)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_negative_samples(df, n_neg=10):\n",
    "    \"\"\"\n",
    "    Generate negative samples (label=0) for implicit feedback data\n",
    "    by randomly picking products the user didn't buy.\n",
    "    \"\"\"\n",
    "    all_products = np.array(df['ProductID'].unique())\n",
    "\n",
    "    neg_samples = []\n",
    "    grouped = df.groupby(\"ClientID\")[\"ProductID\"].apply(set).to_dict()\n",
    "    for client_id, pos_products in grouped.items():\n",
    "        available_neg = np.setdiff1d(all_products, list(pos_products))\n",
    "        if len(available_neg) < n_neg:\n",
    "            sampled_neg = np.random.choice(available_neg, n_neg, replace=True)\n",
    "        else:\n",
    "            sampled_neg = np.random.choice(available_neg, n_neg, replace=False)\n",
    "        # Take one representative row from this user to fill in other feature columns\n",
    "        base_info = df[df[\"ClientID\"] == client_id].iloc[-1].to_dict()\n",
    "        for neg_product in sampled_neg:\n",
    "            row_dict = {**base_info, \"ProductID\": neg_product, \"Label\": 0}\n",
    "            neg_samples.append(row_dict)\n",
    "    return pd.DataFrame(neg_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Step 4.1: Determine the distinct fields\n",
    "# E.g., we group features into \"Client\", \"Product\", \"Interaction\", \"Store\"\n",
    "FIELDS = [\"Client\", \"Product\", \"Store\", \"Interaction\"]  # as an example\n",
    "\n",
    "# We'll build a dictionary like: field -> { original_value -> feature_id }\n",
    "feature_map = {field: {} for field in FIELDS}\n",
    "last_feature_id = 1  # Start assigning IDs at 1\n",
    "\n",
    "def get_feature_id(field, val):\n",
    "    \"\"\"Return a unique integer ID for the given (field, val).\"\"\"\n",
    "    global last_feature_id\n",
    "    if val not in feature_map[field]:\n",
    "        feature_map[field][val] = last_feature_id\n",
    "        last_feature_id += 1\n",
    "    return feature_map[field][val]\n",
    "\n",
    "# Step 4.2: Convert a single row to LibFFM format\n",
    "def row_to_ffm(row):\n",
    "    \"\"\"\n",
    "    row is a dictionary or Series with your feature columns:\n",
    "    - Label\n",
    "    - ClientID, Age, Gender, ...\n",
    "    - ProductID, Brand, ...\n",
    "    - ...\n",
    "    We'll produce a string:  \"<label> f1:featId:val f2:featId:val ...\"\n",
    "    \"\"\"\n",
    "    label = int(row[\"Label\"])  # 1 or 0\n",
    "\n",
    "    ffm_parts = [str(label)]\n",
    "\n",
    "    # ----- Client Field (field=0 in this example) -----\n",
    "    # We'll treat \"ClientID\" as a categorical feature\n",
    "    client_id = get_feature_id(\"Client\", f\"ClientID_{row['ClientID']}\")\n",
    "    ffm_parts.append(f\"0:{client_id}:1\")  # value=1 for this cat. feature\n",
    "\n",
    "    # If you have numeric features in \"Client\", like Age\n",
    "    # You might treat \"Age\" as a separate \"feature\" in the \"Client\" field\n",
    "    # Or you might keep it in \"Interaction\" field—this is up to you\n",
    "    # Example: let's keep Age in \"Client\" field, convert to float\n",
    "    age_val = float(row.get(\"Age\", 30))\n",
    "    age_feat = get_feature_id(\"Client\", \"AGE\")\n",
    "    ffm_parts.append(f\"0:{age_feat}:{age_val}\")\n",
    "\n",
    "    # Similarly for Gender, ClientCountry, etc. you do:\n",
    "    gender_id = get_feature_id(\"Client\", f\"Gender_{row['ClientGender']}\")\n",
    "    ffm_parts.append(f\"0:{gender_id}:1\")\n",
    "\n",
    "    # ----- Product Field (field=1) -----\n",
    "    product_id = get_feature_id(\"Product\", f\"ProductID_{row['ProductID']}\")\n",
    "    ffm_parts.append(f\"1:{product_id}:1\")\n",
    "\n",
    "    brand_id = get_feature_id(\"Product\", f\"Brand_{row['Brand']}\")\n",
    "    ffm_parts.append(f\"1:{brand_id}:1\")\n",
    "\n",
    "    # numeric product_avg_price_order\n",
    "    avgp_feat = get_feature_id(\"Product\", \"product_avg_price_order\")\n",
    "    avgp_val = float(row.get(\"product_avg_price_order\", 0.0))\n",
    "    ffm_parts.append(f\"1:{avgp_feat}:{avgp_val}\")\n",
    "\n",
    "    # ----- Store Field (field=2) -----\n",
    "    store_id = get_feature_id(\"Store\", f\"StoreID_{row['StoreID']}\")\n",
    "    ffm_parts.append(f\"2:{store_id}:1\")\n",
    "\n",
    "    # ----- Interaction Field (field=3) -----\n",
    "    # example: DaysSinceLastTransaction\n",
    "    days_id = get_feature_id(\"Interaction\", \"DaysSinceLastTransaction\")\n",
    "    days_val = float(row.get(\"DaysSinceLastTransaction\", 900))\n",
    "    ffm_parts.append(f\"3:{days_id}:{days_val}\")\n",
    "\n",
    "    # You'd repeat for other numeric columns in \"Interaction\" (cumulativeSpent, frequency, etc.)\n",
    "\n",
    "    return \" \".join(ffm_parts)\n",
    "\n",
    "def df_to_libffm(df, output_file):\n",
    "    \"\"\"\n",
    "    Convert the entire DataFrame to LibFFM format and write to output_file.\n",
    "    This reuses row_to_ffm(...) above.\n",
    "    \"\"\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for _, row in df.iterrows():\n",
    "            ffm_line = row_to_ffm(row)\n",
    "            f.write(ffm_line + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_day = data['TransactionDate'].min().normalize()\n",
    "warmup_end = start_day + pd.Timedelta(days=30)\n",
    "\n",
    "# 5.1 Warm-up Data\n",
    "warmup_data = data[data['TransactionDate'] < warmup_end]\n",
    "warmup_positives = warmup_data.assign(Label=1)\n",
    "warmup_negatives = generate_negative_samples(warmup_data, n_neg=10)\n",
    "warmup_full = pd.concat([warmup_positives, warmup_negatives], ignore_index=True)\n",
    "warmup_full = warmup_full.sample(frac=1, random_state=42).reset_index(drop=True)  # shuffle\n",
    "\n",
    "# Convert to LibFFM format\n",
    "warmup_file = \"warmup_data.ffm\"\n",
    "df_to_libffm(warmup_full, warmup_file)\n",
    "\n",
    "# 5.2 Define and Train\n",
    "model = xl.create_ffm()  # field-aware factorization machine\n",
    "model.setTrain(warmup_file)\n",
    "\n",
    "param = {\n",
    "    \"task\": \"binary\",     # logistic\n",
    "    \"lr\": 0.01,           # learning rate\n",
    "    \"lambda\": 0.00002,    # reg\n",
    "    \"metric\": \"auc\",      # for example\n",
    "    \"epoch\": 5,           # how many passes\n",
    "    \"thread\": 8           # multi-threading\n",
    "}\n",
    "\n",
    "# Train FFM\n",
    "model.fit(param, \"model_out.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_days = pd.date_range(\n",
    "    start=warmup_end.normalize(),\n",
    "    end=data['TransactionDate'].max().normalize(),\n",
    "    freq='W'  # or daily, as you like\n",
    ")\n",
    "\n",
    "current_model = \"model_out.bin\"\n",
    "\n",
    "results = []\n",
    "\n",
    "for day in all_days:\n",
    "    window_start = day - pd.Timedelta(days=30)\n",
    "    if window_start < warmup_end:\n",
    "        window_start = warmup_end\n",
    "\n",
    "    train_subset = data[\n",
    "        (data['TransactionDate'] >= window_start) &\n",
    "        (data['TransactionDate'] <= day)\n",
    "    ]\n",
    "    if train_subset.empty:\n",
    "        continue\n",
    "\n",
    "    # Generate positives + negatives\n",
    "    pos = train_subset.assign(Label=1)\n",
    "    neg = generate_negative_samples(train_subset, n_neg=10)\n",
    "    window_full = pd.concat([pos, neg], ignore_index=True)\n",
    "    window_full = window_full.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Convert to FFM format\n",
    "    train_file = f\"train_{day.strftime('%Y%m%d')}.ffm\"\n",
    "    df_to_libffm(window_full, train_file)\n",
    "\n",
    "    # Create a new xLearn FFM object for the update\n",
    "    model_update = xl.create_ffm()\n",
    "    model_update.setTrain(train_file)\n",
    "\n",
    "    # Resume from current_model\n",
    "    # xLearn calls this \"init_model\"\n",
    "    model_update.setPreModel(current_model)\n",
    "\n",
    "    update_param = {\n",
    "        \"task\": \"binary\",\n",
    "        \"lr\": 0.01,\n",
    "        \"lambda\": 0.00002,\n",
    "        \"metric\": \"auc\",\n",
    "        \"epoch\": 5,\n",
    "        \"thread\": 8,\n",
    "        \"init_model\": current_model  # resume\n",
    "    }\n",
    "\n",
    "    # Retrain or \"continue training\"\n",
    "    model_update.fit(update_param, \"temp_model.bin\")\n",
    "    current_model = f\"model_{day.strftime('%Y%m%d')}.bin\"\n",
    "    os.rename(\"temp_model.bin\", current_model)\n",
    "\n",
    "    # (Pseudo) Evaluate on the same day or next day\n",
    "    # (in practice, you'd convert test set to .ffm, then predict)\n",
    "    # ...\n",
    "    day_result = {\"day\": day, \"accuracy\": 0.0}  # placeholder\n",
    "    results.append(day_result)\n",
    "\n",
    "# Eventually combine results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendations_xlearn(client_id, day, n_top=5, model_path=\"model_out.bin\"):\n",
    "    \"\"\"\n",
    "    1. Construct a DataFrame of (client, product) pairs on the given day.\n",
    "    2. Convert to FFM format.\n",
    "    3. Use xLearn model to predict.\n",
    "    4. Sort by predicted score, return top N.\n",
    "    \"\"\"\n",
    "\n",
    "    # Build the test instances\n",
    "    # e.g., pick relevant products from stocks_df\n",
    "    # Fill in user features from the 'clients_df' or the last known transaction from 'data'\n",
    "    # For each product, create one row with label=0 (dummy), but we only need the fields for scoring\n",
    "    # Then convert to FFM format, call model.predict\n",
    "\n",
    "    pass  # Implementation is similar to \"df_to_libffm\" but for inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
